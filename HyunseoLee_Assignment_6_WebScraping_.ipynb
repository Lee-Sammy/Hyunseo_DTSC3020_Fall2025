{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-Sammy/Hyunseo_DTSC3020_Fall2025/blob/main/HyunseoLee_Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "dbd9061f-65f1-47d2-f6d1-63c604906a89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched ')' (ipython-input-565355372.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-565355372.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1) #Install Required Libraries\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ],
      "source": [
        "1) #Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2505aba-0680-4148-88f1-f9482acf12a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q1_skeleton_answer"
      },
      "outputs": [],
      "source": [
        "# Q1 — Write your answer here\n",
        "\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return the first table with >= 3 columns from the HTML.\n",
        "    Uses pandas.read_html and flattens headers.\n",
        "    \"\"\"\n",
        "    tables = pd.read_html(html)\n",
        "    if not tables:\n",
        "        raise ValueError(\"No tables found in HTML.\")\n",
        "\n",
        "    df = None\n",
        "    for t in tables:\n",
        "        if t.shape[1] >= 3:\n",
        "            df = t.copy()\n",
        "            break\n",
        "\n",
        "    if df is None:\n",
        "        raise ValueError(\"No table with at least 3 columns found.\")\n",
        "\n",
        "    df = flatten_headers(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean columns: strip whitespace, UPPER Alpha-2/Alpha-3,\n",
        "    cast Numeric to nullable int, drop invalid rows.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = flatten_headers(df)\n",
        "\n",
        "    col_map = {}\n",
        "    for c in df.columns:\n",
        "        cl = c.strip().lower()\n",
        "        if \"country\" in cl:\n",
        "            col_map[c] = \"Country\"\n",
        "        elif \"alpha-2\" in cl:\n",
        "            col_map[c] = \"Alpha-2\"\n",
        "        elif \"alpha-3\" in cl:\n",
        "            col_map[c] = \"Alpha-3\"\n",
        "        elif \"numeric\" in cl:\n",
        "            col_map[c] = \"Numeric\"\n",
        "\n",
        "    df = df.rename(columns=col_map)\n",
        "\n",
        "    wanted = [\"Country\", \"Alpha-2\", \"Alpha-3\", \"Numeric\"]\n",
        "    existing = [c for c in wanted if c in df.columns]\n",
        "    df = df[existing]\n",
        "\n",
        "    for col in [\"Country\", \"Alpha-2\", \"Alpha-3\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    for col in [\"Alpha-2\", \"Alpha-3\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].str.upper()\n",
        "\n",
        "    if \"Numeric\" in df.columns:\n",
        "        df[\"Numeric\"] = pd.to_numeric(df[\"Numeric\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    if \"Country\" in df.columns:\n",
        "        df = df[df[\"Country\"].str.strip() != \"\"]\n",
        "    if \"Alpha-2\" in df.columns:\n",
        "        df = df[df[\"Alpha-2\"].str.strip() != \"\"]\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Sort descending by Numeric and return Top-N.\n",
        "    \"\"\"\n",
        "    if \"Numeric\" in df.columns:\n",
        "        return df.sort_values(\"Numeric\", ascending=False).head(top)\n",
        "    else:\n",
        "        return df.head(top)\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.iban.com/country-codes\"\n",
        "\n",
        "html = fetch_html(url)\n",
        "df_raw = q1_read_table(html)\n",
        "df_clean = q1_clean(df_raw)\n",
        "\n",
        "# Top-15 sorted by Numeric desc\n",
        "df_top15 = q1_sort_top(df_clean, top=15)\n",
        "print(df_top15)\n",
        "\n",
        "# Save full cleaned data\n",
        "df_clean.to_csv(\"data_q1.csv\", index=False)\n",
        "print(\"\\nSaved data_q1.csv with\", len(df_clean), \"rows.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o7uCWFzYfjS",
        "outputId": "e4a80907-89c3-4c13-ecc7-269d3fb2dcf0"
      },
      "id": "1o7uCWFzYfjS",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Country Alpha-2 Alpha-3  \\\n",
            "247                                             Zambia      ZM     ZMB   \n",
            "246                                              Yemen      YE     YEM   \n",
            "192                                              Samoa      WS     WSM   \n",
            "244                                  Wallis and Futuna      WF     WLF   \n",
            "240                 Venezuela (Bolivarian Republic of)      VE     VEN   \n",
            "238                                         Uzbekistan      UZ     UZB   \n",
            "237                                            Uruguay      UY     URY   \n",
            "35                                        Burkina Faso      BF     BFA   \n",
            "243                              Virgin Islands (U.S.)      VI     VIR   \n",
            "236                     United States of America (the)      US     USA   \n",
            "219                       Tanzania, United Republic of      TZ     TZA   \n",
            "108                                        Isle of Man      IM     IMN   \n",
            "113                                             Jersey      JE     JEY   \n",
            "92                                            Guernsey      GG     GGY   \n",
            "234  United Kingdom of Great Britain and Northern I...      GB     GBR   \n",
            "\n",
            "     Numeric  \n",
            "247      894  \n",
            "246      887  \n",
            "192      882  \n",
            "244      876  \n",
            "240      862  \n",
            "238      860  \n",
            "237      858  \n",
            "35       854  \n",
            "243      850  \n",
            "236      840  \n",
            "219      834  \n",
            "108      833  \n",
            "113      832  \n",
            "92       831  \n",
            "234      826  \n",
            "\n",
            "Saved data_q1.csv with 249 rows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2698840710.py:8: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q2_skeleton_answer"
      },
      "outputs": [],
      "source": [
        "# Q2 — Write your answer here\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    rows = []\n",
        "\n",
        "    for item in soup.select(\"tr.athing\"):\n",
        "        rank_el = item.select_one(\".rank\")\n",
        "        rank_text = rank_el.get_text(strip=True) if rank_el else \"\"\n",
        "        rank_digits = re.findall(r\"\\d+\", rank_text)\n",
        "        rank = int(rank_digits[0]) if rank_digits else 0\n",
        "\n",
        "        title_link = None\n",
        "        title_el = item.select_one(\".titleline a\")\n",
        "        if title_el:\n",
        "            title = title_el.get_text(strip=True)\n",
        "            link = title_el.get(\"href\", \"\").strip()\n",
        "        else:\n",
        "            title_el = item.select_one(\"a.storylink\")\n",
        "            title = title_el.get_text(strip=True) if title_el else \"\"\n",
        "            link = title_el.get(\"href\", \"\").strip() if title_el else \"\"\n",
        "\n",
        "        sub = item.find_next_sibling(\"tr\")\n",
        "        points = 0\n",
        "        comments = 0\n",
        "        user = \"\"\n",
        "\n",
        "        if sub:\n",
        "            subtext = sub.select_one(\".subtext\")\n",
        "            if subtext:\n",
        "                score_el = subtext.select_one(\".score\")\n",
        "                if score_el:\n",
        "                    score_text = score_el.get_text(strip=True)  # \"123 points\"\n",
        "                    score_digits = re.findall(r\"\\d+\", score_text)\n",
        "                    points = int(score_digits[0]) if score_digits else 0\n",
        "\n",
        "                user_el = subtext.select_one(\"a.hnuser\")\n",
        "                if user_el:\n",
        "                    user = user_el.get_text(strip=True)\n",
        "\n",
        "                a_tags = subtext.select(\"a\")\n",
        "                if a_tags:\n",
        "                    last_a = a_tags[-1]\n",
        "                    c_text = last_a.get_text(strip=True)\n",
        "                    c_digits = re.findall(r\"\\d+\", c_text)\n",
        "                    comments = int(c_digits[0]) if c_digits else 0\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"title\": title,\n",
        "                \"link\": link,\n",
        "                \"points\": points,\n",
        "                \"comments\": comments,\n",
        "                \"user\": user,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    for col in [\"rank\", \"points\", \"comments\"]:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    for col in [\"title\", \"link\", \"user\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    if \"points\" in df.columns:\n",
        "        return df.sort_values(\"points\", ascending=False).head(top)\n",
        "    else:\n",
        "        return df.head(top)\n",
        "\n",
        "\n"
      ],
      "id": "q2_skeleton_answer"
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://news.ycombinator.com/\"\n",
        "\n",
        "html = fetch_html(url)\n",
        "df_raw_q2 = q2_parse_items(html)\n",
        "df_clean_q2 = q2_clean(df_raw_q2)\n",
        "\n",
        "df_top15_q2 = q2_sort_top(df_clean_q2, top=15)\n",
        "print(df_top15_q2)\n",
        "\n",
        "df_clean_q2.to_csv(\"data_q2.csv\", index=False)\n",
        "print(\"\\nSaved data_q2.csv with\", len(df_clean_q2), \"rows.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POqhKIiuZDhS",
        "outputId": "11ff29a8-3f66-488f-f1a9-a8988a8de4ea"
      },
      "id": "POqhKIiuZDhS",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    rank                                              title  \\\n",
            "14    15            FBI tries to unmask owner of archive.is   \n",
            "4      5  Kimi K2 Thinking, a SOTA open-source trillion-...   \n",
            "22    23             ICC ditches Microsoft 365 for openDesk   \n",
            "0      1                          You should write an agent   \n",
            "10    11  Open Source Implementation of Apple's Private ...   \n",
            "5      6           Two billion email addresses were exposed   \n",
            "25    26  I may have found a way to spot U.S. at-sea str...   \n",
            "6      7  Show HN: I scraped 3B Goodreads reviews to tra...   \n",
            "20    21    Mathematical exploration and discovery at scale   \n",
            "9     10                           Swift on FreeBSD Preview   \n",
            "15    16                            Eating stinging nettles   \n",
            "3      4                              Game design is simple   \n",
            "17    18  I analyzed the lineups at the most popular nig...   \n",
            "28    29  Show HN: qqqa – A fast, stateless LLM-powered ...   \n",
            "11    12             LLMs encode how difficult problems are   \n",
            "\n",
            "                                                 link  points  comments  \\\n",
            "14  https://www.heise.de/en/news/Archive-today-FBI...     736       383   \n",
            "4   https://moonshotai.github.io/Kimi-K2/thinking....     616       253   \n",
            "22  https://www.binnenlandsbestuur.nl/digitaal/int...     541       173   \n",
            "0        https://fly.io/blog/everyone-write-an-agent/     404       199   \n",
            "10                 https://github.com/openpcc/openpcc     364        79   \n",
            "5   https://www.troyhunt.com/2-billion-email-addre...     359       245   \n",
            "25  https://old.reddit.com/r/OSINT/comments/1opjjy...     313       463   \n",
            "6                                     https://book.sv     277       100   \n",
            "20  https://terrytao.wordpress.com/2025/11/05/math...     232       115   \n",
            "9   https://forums.swift.org/t/swift-on-freebsd-pr...     188       111   \n",
            "15  https://rachel.blog/2018/04/29/eating-stinging...     181       170   \n",
            "3   https://www.raphkoster.com/2025/11/03/game-des...     160        52   \n",
            "17  https://dev.karltryggvason.com/how-i-analyzed-...     142        69   \n",
            "28                  https://github.com/matisojka/qqqa     132        80   \n",
            "11                   https://arxiv.org/abs/2510.18147     114        21   \n",
            "\n",
            "               user  \n",
            "14     Projectiboga  \n",
            "4         nekofneko  \n",
            "22         vincvinc  \n",
            "0       tabletcorry  \n",
            "10   adam_gyroscope  \n",
            "5            esnard  \n",
            "25          hentrep  \n",
            "6            costco  \n",
            "20           nabla9  \n",
            "9          glhaynes  \n",
            "15              rzk  \n",
            "3             vrnvu  \n",
            "17            kalli  \n",
            "28          iagooar  \n",
            "11  stansApprentice  \n",
            "\n",
            "Saved data_q2.csv with 30 rows.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}